misValperc <- arguments[i_exp,2]
print(dim(Xtemp))
print(dim(Ytemp))
#Xtemp_mis <- prodNA(Xtemp, noNA = misValperc/100)
# Here we pass ONLY the subset of rows/columns [n,k] for imputation and scaling of Xtemp [training/learning]
# If M subsets have been already generated offline, then a RData workspace will be loaded
# with the correspondent imputed/normalized X and Xpred
Xtemp_mis <- Xtemp[n,k]
# Here we pass ONLY the subset of columns [,k] for imputation and scaling of Xpred [validation/preditcion]
Xpred_mis <- Xpred[,k]
# Here I impute the missing data with the function missForest
Xtemp_imp <- missForest(Xtemp_mis, maxiter = 5)
Xpred_imp <- missForest(Xpred_mis, maxiter = 5)
## DATA NORMALIZATION of the sampled matrix without Group and Sex
## This step can be generalized if the data is formatted RAW,
# with categorical variables as binary or strings (see commented out example below)
# a1 = which(names(Xtemp_imp$ximp) == "Group")
# a2 = which(names(Xtemp_imp$ximp) == "Sex")
# cont = 1:length(Xtemp_imp$ximp)
# cont <- cont[-1*c(a1,a2)]
# # DATA NORMALIZATION if IMPUTATION IS PERFORMED
#Xnorm_ALL <- as.data.frame(scale(Xtemp_imp$ximp))
Xtemp_norm <- as.data.frame(scale(Xtemp_imp$ximp))
Xpred_norm <- as.data.frame(scale(Xpred_imp$ximp))
# STEPS 5 and 6 ADD LIBRARIES
# Specify new SL prediction algorithm wrappers
SL.glmnet.0 <- function(..., alpha = 0,family="binomial"){
SL.glmnet(..., alpha = alpha , family = family)
} # ridge penalty
SL.glmnet.1 <- function(..., alpha = 1,family="binomial"){
SL.glmnet(..., alpha = alpha , family = family)
} # ridge penalty
SL.glmnet.0.25 <- function(..., alpha = 0.25,family="binomial"){
SL.glmnet(..., alpha = alpha, family = family)
}
SL.glmnet.0.50 <- function(..., alpha = 0.50,family="binomial"){
SL.glmnet(..., alpha = alpha, family = family)
}
SL.glmnet.0.75 <- function(..., alpha = 0.75,family="binomial"){
SL.glmnet(..., alpha = alpha, family = family)
}
SL.gam.1<-function(...,control=gam.control(deg.gam=1)){
SL.gam(...,control=control)
}
SL.gam.3<-function(...,control=gam.control(deg.gam=3)){
SL.gam(...,control=control)
}
SL.gam.4<-function(...,control=gam.control(deg.gam=4)){
SL.gam(...,control=control)
}
SL.gam.5<-function(...,control=gam.control(deg.gam=5)){
SL.gam(...,control=control)
}
create.SL.glmnet.alpha<-function(...,alpha=c(0,0.25,0.5,0.75,1))
{
SL.glmnet(..., alpha=alpha)
}
## The bartMachine wrapper won't be necessary with the latest release of the SL.bartMachine.
## It's not properly installed yet.
#' Wrapper for bartMachine learner
#'
#' Support bayesian additive regression trees via the bartMachine package.
#'
#' @param Y Outcome variable
#' @param X Covariate dataframe
#' @param newX Optional dataframe to predict the outcome
#' @param obsWeights Optional observation-level weights (supported but not tested)
#' @param id Optional id to group observations from the same unit (not used
#'   currently).
#' @param family "gaussian" for regression, "binomial" for binary
#'   classification
#' @param num_trees The number of trees to be grown in the sum-of-trees model.
#' @param num_burn_in Number of MCMC samples to be discarded as "burn-in".
#' @param num_iterations_after_burn_in Number of MCMC samples to draw from the
#'   posterior distribution of f(x).
#' @param alpha Base hyperparameter in tree prior for whether a node is
#'   nonterminal or not.
#' @param beta Power hyperparameter in tree prior for whether a node is
#'   nonterminal or not.
#' @param k For regression, k determines the prior probability that E(Y|X) is
#'   contained in the interval (y_{min}, y_{max}), based on a normal
#'   distribution. For example, when k=2, the prior probability is 95\%. For
#'   classification, k determines the prior probability that E(Y|X) is between
#'   (-3,3). Note that a larger value of k results in more shrinkage and a more
#'   conservative fit.
#' @param q Quantile of the prior on the error variance at which the data-based
#'   estimate is placed. Note that the larger the value of q, the more
#'   aggressive the fit as you are placing more prior weight on values lower
#'   than the data-based estimate. Not used for classification.
#' @param nu Degrees of freedom for the inverse chi^2 prior. Not used for
#'   classification.
#' @param verbose Prints information about progress of the algorithm to the
#'   screen.
#' @param ... Additional arguments (not used)
#'
#' @encoding utf-8
#' @export
SL.bartMachine <- function(Y, X, newX, family, obsWeights, id,
num_trees = 50, num_burn_in = 250, verbose = F,
alpha = 0.95, beta = 2, k = 2, q = 0.9, nu = 3,
num_iterations_after_burn_in = 1000,
...) {
#.SL.require("bartMachine")
model = bartMachine::bartMachine(X, Y, num_trees = num_trees,
num_burn_in = num_burn_in, verbose = verbose,
alpha = alpha, beta = beta, k = k, q = q, nu = nu,
num_iterations_after_burn_in = num_iterations_after_burn_in)
# pred returns predicted responses (on the scale of the outcome)
pred <- predict(model, newX)
# fit returns all objects needed for predict.SL.template
fit <- list(object = model)
#fit <- vector("list", length=0)
class(fit) <- c("SL.bartMachine")
out <- list(pred = pred, fit = fit)
return(out)
}
#' bartMachine prediction
#' @param object SuperLearner object
#' @param newdata Dataframe to predict the outcome
#' @param family "gaussian" for regression, "binomial" for binary
#'   classification. (Not used)
#' @param Y Outcome variable (not used)
#' @param X Covariate dataframe (not used)
#' @param ... Additional arguments (not used)
#'
#' @export
predict.SL.bartMachine <- function(object, newdata, family, X = NULL, Y = NULL,...) {
pred <- predict(object$object, newdata)
return(pred)
}
# SL.library <- c("SL.glm","SL.gam","SL.gam.1","SL.gam.3","SL.gam.4","SL.gam.5",
#                 "SL.glmnet","SL.glmnet.0","SL.glmnet.0.25","SL.glmnet.0.50","SL.glmnet.0.75","SL.glmnet.1",
#                 "SL.svm",
#                 "SL.randomForest","SL.bartMachine")
SL.library <- c("SL.glm",
"SL.glmnet","SL.glmnet.0","SL.glmnet.0.25","SL.glmnet.0.50","SL.glmnet.0.75","SL.glmnet.1",
"SL.svm","SL.randomForest","SL.bartMachine")
# Automated labeling of sub-matrices, assigned to X
X <- as.data.frame(Xtemp_norm)
Y <- Ytemp[n]
eval(parse(text=paste0("k",j_global," <- k")))
eval(parse(text=paste0("n",j_global," <- n")))
## KNOCKOFF FILTER IMPLEMENTATION
## IMPORTANT  --> subjects # >> features # !!!
## It creates KO_result_j objects with all the stats, results, FDR proportion,...
# knockoff.filter(X, Y, fdr = 0.2, statistic = NULL,
# threshold = c("knockoff", "knockoff+"), knockoffs = c("equicorrelated","sdp"),
#               normalize = TRUE, randomize = FALSE)
if (dim(X)[2]<dim(X)[1])
{
eval(parse(text=paste0("KO_result_",j_global," = knockoff.filter(X,Y,fdr = 0.05)")))
eval(parse(text=paste0("KO_selected_",j_global," <- as.numeric(sub(\"V\",\"\",names(KO_result_",j_global,"$selected)))")))
eval(parse(text=paste0("print(KO_selected_",j_global,")")))
} else {
eval(parse(text=paste0("print(KO_selected_",j_global,"<-NULL)")))
}
## SUPERLEARNER LOOP
# SUPERLEARNER-SL FUNCTION CALL that generates SL objects
## Superlearner Function - Accuracy Inference ##
print(c("Dimension of X"))
print(dim(X))
print(dim(Xpred_norm))
SL <- try(SuperLearner(Y , X , newX = Xpred_norm,
family=binomial(),
SL.library=SL.library,
method="method.NNLS",
verbose = FALSE,
control = list(saveFitLibrary = TRUE),
cvControl = list(V=10)));
SL_Pred <- data.frame(prediction = SL$SL.predict[, 1])
Classify <-  NULL;
Classify_MSE <- NULL
for (i in 1:dim(SL_Pred)[1])
{
ifelse((SL_Pred[i,] > 0.5),Classify[i] <- 1,Classify[i] <- 0)
}
Classify_MSE <- apply(SL_Pred, 1, function(xx) xx[unname(which.max(xx))])
eval(parse(text=paste0("SL_Pred_",j_global," <- Classify")))
eval(parse(text=paste0("SL_Pred_MSE_",j_global," <- Classify_MSE")))
# This checks if the SL_Pred object was successfully generated (i.e., if it exists)
# If it does not exist, it is set to a double equal to 100
eval(parse(text=paste0("ifelse(exists(\"SL_Pred_",j_global,"\"),'OK',
SL_Pred_",j_global," <- 100)")))
truth=Ypred; # set the true outcome to be predicted
pred=Classify
# pred_temp <- pred;pred_temp[which(pred_temp == "MCI/LMCI")] <- c("MCI")
# truth_temp <- truth;truth_temp[which(truth_temp == "LMCI")] <- c("MCI")
cmatrix <- confusionMatrix(pred,truth)
# This sets the "MSE_jglobal" to the ACCURACY of the prediction (i.e., cmatrix$overall[1])
# The final ranking will then be done on the accuracy rather than
# on a distance-based metrics. A real MSE calculation might be OK for continuous outcomes
eval(parse(text=paste0("Accuracy_",j_global,"<- unname(cmatrix$overall[1])")))
## MSE GENERATION
sum=0
for(s in 1:length(Ypred)) {
## This checks first if the TYPE of the prediction object SL_Pred is NOT a double (which means that
## the prediction step worked in the previous module. If it is NOT a double, the MSE is calculated
## as the mean of the sum of the square of the difference between the prediction and the data to predict.
# If the SL_Pred is a double, SL_Pred_j is assigned to its MSE (very high value).
# eval(parse(text=paste0("ifelse(typeof(SL_Pred_MSE_",j_global,") != \"double\",
#                        sum <- sum+sum(Ypred[",s,"] - SL_Pred_MSE_",j_global,"$pred[",s,"])^2,sum <- SL_Pred_MSE_",j_global,")")))
#ifelse(typeof(Classify_MSE) != "double",sum <- sum+sum(Ypred[s] - Classify_MSE[s])^2,sum <- Classify_MSE)
#ifelse(exists("Classify_MSE") ,sum <- sum+sum(Ypred[s] - Classify_MSE[s])^2,sum <- Classify_MSE)
ifelse(exists("Classify_MSE") ,sum <- sum+sum(Ypred[s] - Classify_MSE[s])^2,sum <- 100)
}
# eval(parse(text=paste0("ifelse(exists(\"SL_Pred_",j_global,"\"),'OK',
#                  SL_Pred_",j_global," <- 100)")))
eval(parse(text=paste0("ifelse(exists(\"SL_Pred_MSE_",j_global,"\"),sum <- sum+sum(Ypred[s] - SL_Pred_MSE_",j_global,"[s])^2,
sum <- 10000)")))
## This step makes the final calculation of the MSE and it labels it with a j --> MSE_j
eval(parse(text=paste0("MSE_",j_global," <- sum/length(Ypred)")))
## This resets the sum to 0 for the next MSE calculation
# GENERATE THE LIGHT DATASET BY DELETING THE SL OBJECT
rm(SL)
#nonzero=c(10,20,30,40,50,60,70,80,90,100)
nonzero=c(1,30,60,100,130,160,200,230,260,300)
# nonzero=c(1,100,200,300,400,500,600,700,800,900)
# nonzero=c(1,100,200,400,600,800,1000,1200,1400,1500)
# SAVE THE RDATA WORKSPACE WITH THE ALL DATA
# eval(parse(text=paste0("save(Xtemp,Xpred,dataset_file, q, Ypred, M, i_exp, Ytemp, SL_Pred_MSE_",j_global,",SL_Pred_",j_global,
#                        ",nonzero,workspace_directory,k,label,",j_global,",MSE_",j_global,",Accuracy_",j_global,",KO_selected_",j_global,",
#                        file= \"",workspace_directory,"CBDA_SL_M",M,"_miss",misValperc,"_n",range_n,"_k"
#                        ,range_k,"_Light_",j_global,"_",label,".RData\")")))
eval(parse(text=paste0("save.image(file= \"",workspace_directory,"CBDA_Inference_",j_global,"_",label,".RData\")")))
eval(parse(text=paste0("print(MSE_",j_global,")")))
eval(parse(text=paste0("print(Accuracy_",j_global,")")))
# This is enforced in case a pdf channel has been opened at the beginning (line 5)
# to save results in a pdf
dev.off()
# for(j_global in 1:100)
# {
#   eval(parse(text=paste0("load(file= \"",workspace_directory,"CBDA_Inference_",j_global,"_",label,".RData\")")))
#   eval(parse(text=paste0("qa_ALL$Inference_Acc[",j_global,"] <- Accuracy_",j_global,"")))
#   eval(parse(text=paste0("qa_ALL$Inference_MSE[",j_global,"] <- MSE_",j_global,"")))
# }
# print(qa_ALL$Inference_Acc)
# print(qa_ALL$Inference_MSE)
# eval(parse(text=paste0("save(arguments,label,workspace_directory,i_exp,arg_file,
#                        file= \"~/",label,"_info_for_consolidation.RData\")")))
#CV Superlearner function application [NOT TESTED YET]
# CV_SL <- try(CV.SuperLearner(Y,
#                              X,
#                              V=10, family=gaussian(),
#                              SL.library=SL.library,
#                              method="method.NNLS",
#                              verbose = TRUE,
#                              control = list(saveFitLibrary = TRUE),
#                              cvControl = list(V=10), saveAll = TRUE));#,
#                              #parallel = 'multicore'));
#
# eval(parse(text=paste0("CV_SL_",j_global,"_KO <- CV_SL")));
#
# eval(parse(text=paste0("ifelse(exists(\"CV_SL_",j_global,"_KO\"),'OK',
#                        CV_SL_",j_global,"_KO <- 1)")))
#
# eval(parse(text=paste0("save(Xnew,Ynew,CV_SL_",j_global,"_KO,k",j_global,",n",j_global,",file= \"",
#                        workspace_directory,"CBDA_CV_SL_M",M,"_miss",misValperc,"_n",range_n,"_k",range_k,"_",j_global,"_KO.RData\")")))
eval(parse(text=paste0("save.image(file= \"",workspace_directory,"CBDA_Inference_",j_global,"_",label,".RData\")")))
workspace_directory
j_global
4.9+3.48+3.35+4.72+7.15
x = c(0.4,0.6,0,0.2,0.5,0.3,0.1,0.7,0.2)
dim(x) = c(3,3)
x
x^2
x%*%x
x%*%x%*%x%*%x
x%*%x%*&x
x%*%x%*%x%*%x%*%x%*%x%*%x%*%x
x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x
x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x
x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x
x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x
x%*%x%*%x%*%x
x%*%x%*%x%*%x%*%x%*%x%*%x%*%x
x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x
x
x=t(x)
x
x%*%x%*%x%*%x
x%*%x%*%x%*%x%*%x%*%x%*%x%*%x
x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x
49*9737/9951/263
23*9928/9977/72
(263+72)*(9951+9977)/(49+23)/(9928+9737)
19*52/11/132
x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%
a = c(192,75,8,459,586,471)
x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%
a = c(192,75,8,459,586,471)
x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%x%*%
a = c(192,75,8,459,586,471)
a = c(192,75,8,459,586,471)
dim(a) = c(2,3)
a
dim(a) = c(3,2)
a
t(a)
a = t(a)
rowSums(a)
a/rowSums(a)
a[1:]
a[1:, ]
a[1,: ]
a[1,:]
a[1,]
a[1, ]
sum(a)
func = function(i,j){
sum(a[i, ])*sum(a[ ,j])/sum(a)
}
func(1,1)
250*651/1791
275*651/1791
func1 = function(i,j){
(a[i,j] - func(i,j))^2/func(i,j)
}
func1(1,1)
(192 - 99.958)^2/99.958
b = 0
for i in c(1,2){
for j in c(1,2,3){
b = b+func1(i,j)
}
}
for (i in c(1,2)){
for (j in c(1,2,3)){
b = b+func1(i,j)
}
}
b
?chisq
chisq.test()
chisq.test
?chisq.test
qchisq
?qchisq
pchisq(100)
pchisq(100,df = 1)
pchisq(0,df = 1)
qchisq(0.95,2)
func(1,1)
a[1,1] - func(1,1)/(func(1,1)*(1-275/sum(a))*(1-651/sum(a)))
a[1,1] - func(1,1)/(func(1,1)*(1-275/sum(a))*(1-651/sum(a)))^(1/2)
a[1,1] - func(1,1)/sqrt(func(1,1))
(a[1,1] - func(1,1))/(func(1,1)*(1-275/sum(a))*(1-651/sum(a)))^(1/2)
a= c(192,75,459,586)
dim(a) = c(3,2)
a = t(a)
a= c(192,75,459,586)
dim(a) = c(2,2)
a = t(a)
a
func = function(i,j){
sum(a[i, ])*sum(a[ ,j])/sum(a)
}
func1 = function(i,j){
(a[i,j] - func(i,j))^2/func(i,j)
}
b = 0
for (i in c(1,2)){
for (j in c(1,2)){
b = b+func1(i,j)
}
}
b
func2 - function(i,j){
a[i,j]*log(a[i,j]/func(i,j))
}
b = 0
for (i in c(1,2)){
for (j in c(1,2)){
b = b+func2(i,j)
}
}
func2 = function(i,j){
a[i,j]*log(a[i,j]/func(i,j))
}
b = 0
for (i in c(1,2)){
for (j in c(1,2)){
b = b+func2(i,j)
}
}
b
b*2
16/8/7/6/5
16/8/7/6/5*4*3*2
0.88*0.2*0.1*0.448
setwd("/Users/zhao_fengyu/Desktop/课件/BIO 615/hw")
library(Rcpp)## make sure to change the path below to point the correct path
sourceCpp("hw3a.cpp")
sourceCpp("hw3c.cpp")
choose_fac(10,5)
choose_dp(10,5)
choose_fac(15,5)
choose_dp(15,5)
choose(15,5)
choose_fac(10,5)
choose_dp(10,5)
choose(10,5)
choose_fac(15,5)
choose_dp(15,5)
choose(15,5)
choose_fac(20,10)
choose_dp(20,10)
choose(20,10)
choose_fac(30,15)
choose_dp(30,15)
choose(30,15)
choose_fac(60,30)
choose_dp(60,30)
choose(60,30)
int fac(int n) {
if ( n < 2 ) return 1; // return 1 when n=0,1
int ret = 1;
for(int i=2; i <= n; ++i)
ret *= i;  // calculate factorial
return ret;
}
factorial(15)
sourceCpp("hw3a.cpp")
choose_fac(60,30)
choose_dp(60,30)
choose(60,30)
choose(60,30)
sourceCpp("hw3a.cpp")
choose_dp(60,30)
choose(60,30)
factorial(30)
factorial(50)
factorial(80)
factorial(100)
choose_fac(10,5)
choose_dp(10,5)
choose(10,5)
choose_fac(15,5)
choose_dp(15,5)
choose(15,5)
choose_fac(20,10)
choose_dp(20,10)
choose(20,10)
choose_fac(30,15)
choose_dp(30,15)
choose(30,15)
choose_fac(60,30)
choose_dp(60,30)
choose(60,30)
factorial(15)
sourceCpp("fengyuz_hw3c.cpp")
alignWords("FOOD","MONEY",1,1)
alignWords("FOOD","MONEY",3,1)
alignWords("ALGORITHM","ALTRUISTIC",1,1)
alignWords("ALGORITHM","ALTRUISTIC",2,1)
0.0172*(1-0.0172)
log(0.017)
log(0.021)
sourceCpp("fengyuz_hw3a.cpp")
choose_fac(10,5)
choose_dp(10,5)
choose(10,5)
choose_fac(15,5)
choose_dp(15,5)
choose(15,5)
choose_fac(20,10)
choose_dp(20,10)
choose(20,10)
choose_fac(30,15)
choose_dp(30,15)
choose(30,15)
choose_fac(60,30)
choose_dp(60,30)
choose(60,30)
sourceCpp("fengyuz_hw3a.cpp")
choose_fac(10,5)
choose_dp(10,5)
choose(10,5)
choose_fac(15,5)
choose_dp(15,5)
choose(15,5)
choose_fac(20,10)
choose_dp(20,10)
choose(20,10)
choose_fac(30,15)
choose_dp(30,15)
choose(30,15)
choose_fac(60,30)
choose_dp(60,30)
choose(60,30)
sourceCpp("fengyuz_hw3c.cpp")
alignWords("FOOD","MONEY",1,1)
alignWords("FOOD","MONEY",3,1)
alignWords("ALGORITHM","ALTRUISTIC",1,1)
alignWords("ALGORITHM","ALTRUISTIC",2,1)
20/1.1+50/1.1^2+50/1.1^3
(1+0.038)^(360/175)
(1+0.038)^(360/175)^1/2
(1+0.038)^(360/175)^(1/2)
(1+0.038)^(175/360)
(1+0.038)^(175/360)-1
0.01829524*360/175
(1+0.038)^(175/365)-1
0.01804237*360/175
0.01804237*10000
0.01804237*10000/(10000-0.01804237*10000)
(10000-0.01804237*10000)/10000* 0.01804237
0.01771684*360/175
1.22*1.1
1.342^0.5
103.6-2.072
1.14*0.9*0.98
1.00548^0.33333
